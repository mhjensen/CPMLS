# FYS4447/9447 – Advanced machine learning and data analysis for the physical sciences


### Level Master and PhD
### Credits 10
### Teaching spring semester
### Examination Every Spring
### Teaching language Norwegian (English on request)


## Course content

Advances in artificial intelligence/machine learning methods provide
tools that have broad applicability in scientific research.  These
techniques are being applied across the diversity of research topics
in modern science, leading to advances that will facilitate scientific
discoveries and societal applications.  This course focuses on
advanced machine learning and statistical learning methods applied to
a broad variety of problems in the physical sciences and life science,
from computational neuroscience to the analysis of high-energy physics
experiments.  Supervised and unsupervised learning methods are
discussed, spanning from various deep learning methods to Bayesian modeling. 


After this course you should :

- be familiar with central deep learning methods and how to use them in actual research
- be familiar with advanced regression algorithms
- understand how to to simulate complex physical processes with many degrees of freedom
- understand optimization techniques and their fundamental role in machine learning
- be familiar with Bayesian statistics and Bayesian Machine Learning
- understand how to find correlations in data sets and quantify uncertainties
- understand how to use Gaussian processes in the analysis of physics problems
## Admission

Students admitted at UiO must apply for courses in
Studentweb. Students enrolled in other Master's Degree Programmes can,
on application, be admitted to the course if this is cleared by their
own study programme.

Nordic citizens and applicants residing in the Nordic countries may
apply to take this course as a single course student.

If you are not already enrolled as a student at UiO, please see our
information about admission requirements and procedures for
international applicants.

## Prerequisites

Recommended previous knowledge

A good background in mathematics is needed.

Other recommended courses are:

FYS-STK4155 – Applied Data Analysis and Machine Learning
IN5400 - Machine learning for image analysis

## Overlapping courses
None


## Teaching

The course is based on assigned self studies and three projects. There are no regular lectures. Meetings every week (lasting two hours) are arranged in order to monitor progress and discuss projects and exercises.

## Examination

Three project assignments that each is given a weight of 1/3 in the final grading (100% together).
There is no final exam.


### Examination support material

No examination support material is allowed.

## Language of examination

You may write your examination papers in Norwegian, Swedish, Danish or English.

## Grading scale

Grades are awarded on a scale from A to F, where A is the best grade and F is a fail. Read more about the grading system. For PhD students it is only pass/not passed.



## Detailed content (this may vary according to the interests of the participants, but what is listed here covers the main topics).

# Advanced deep learning methods

Convolutional
10.1. The structure of convolutional neural networks ................................................................................................................... 61
High-level concepts in deep neural networks .................................................................................................................................... 64
Organizing deep learning workflows using the bias–variance tradeoff...................................................
Why neural networks are so successful: three high-level perspectives on neural networks.......................................... 66
11.2.1. Neural networks as representation learning .......................................................................................................... 66
11.2.2. Neural networks can exploit large amounts of data ............................................................................................. 66
11.2.3. Neural networks scale up well computationally.................................................................................................... 67
Neural Networks (CNNS) .............................................................................................................................................. 61
11.3.
Dimensional reduction and data visualization .........................................................................
12.2. Principal component analysis (PCA) ........................................................................................................................................ 69
12.3. Multidimensional scaling .......................................................................................
Clustering ................................................................................................................................................................................................ 73
16.
Summary of the training procedure ........................................................................................................................................ 93 Deep generative models: Hidden variables and restricted Boltzmann machines (RBMs)............................................................. 94
17.
16.4. 16.5.
16.6.
17.1. 17.2. 17.3.
17.4.
18.
Outlook .................................................................................................................................................................................................... 112
18.1. Research at the intersection of physics and ML.................................................................................................................... 114
18.2. Topics not covered in review................................................................................................................................................... 114
18.3. Rebranding machine learning as ‘‘Artificial Intelligence’’..................................................................................................... 115
18.4. Social implications of machine learning ................................................................................................................................. 116
Acknowledgments .................................................................................................................................................................................. 116 Appendix. Overview of the datasets used in the review .................................................................................................................. 116
A.1. Ising dataset ............................................................................................................................................................................... 116
A.2. SUSY Dataset .............................................................................................................................................................................. 116
A.3. MNIST Dataset............................................................................................................................................................................ 117
References ............................................................................................................................................................................................... 117
13.1.




13.
14.
15.
12.2. Principal component analysis (PCA) ........................................................................................................................................ 69
12.3. Multidimensional scaling .......................................................................................................................................................... 71
12.4. t-SNE .......................................................................................................................................................................................... 71
Practical clustering methods .................................................................................................................................................... 75
13.1.1. K -Means ...................................................................................................................................................................... 75
13.1.2. Hierarchical clustering: Agglomerative methods ................................................................................................... 76
13.1.3. Density-based (DB) clustering .................................................................................................................................. 77
Clustering and latent variables via the Gaussian mixture models ..................................................................................... 79

Clustering in high dimensions ................................................................................................................................................ 81
Variational methods and mean-field theory (MFT)............................................................................................................................ 82
14.1. Variational mean-field theory for the Ising model................................................................................................................ 83
14.2. Expectation–Maximization (EM) .............................................................................................................................................. 85
Energy based models: Maximum entropy (MaxEnt) principle, generative models, and Boltzmann learning............................ 87
An overview of energy-based generative models.................................................................................................................. 88
Maximum entropy models: the simplest energy-based generative models ...................................................................... 89
15.2.1. MaxEnt models in statistical mechanics ................................................................................................................. 89
15.2.2. From statistical mechanics to machine learning .................................................................................................... 90
Why hidden (latent) variables? ............................................................................................................................................... 94 Restricted Boltzmann machines (RBMs) ................................................................................................................................. 95 Training RBMs ............................................................................................................................................................................ 97
16.3.1. Gibbs sampling and contrastive divergence (CD) .................................................................................................. 97 16.3.2. Practical considerations ............................................................................................................................................. 98 Deep Boltzmann machine ......................................................................................................................................................... 99 Generative models in practice: examples............................................................................................................................... 100
16.5.1. MNIST .......................................................................................................................................................................... 100
16.5.2. Example: 2D Ising model.......................................................................................................................................... 100
Generative models in physics .................................................................................................................................................. 101 Variational autoencoders (VAEs) and generative adversarial networks (GANs)............................................................................. 102
The limitations of maximizing likelihood ............................................................................................................................... 103 Generative models and adversarial learning ......................................................................................................................... 104 Variational Autoencoders (VAEs) ............................................................................................................................................. 106
17.3.1. VAEs as variational models ....................................................................................................................................... 106
17.3.2. Training via the reparametrization trick ................................................................................................................. 107
17.3.3. Connection to the information bottleneck .............................................................................................................. 109
VAE with Gaussian latent variables and Gaussian encoder.................................................................................................. 110
17.4.1. Implementing the Gaussian VAE .............................................................................................................................. 110
17.4.2. VAEs for the MNIST dataset .................................................................................................................................... 111
